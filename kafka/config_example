
@EnableKafka
@Configuration
public class KafkaConfiguration {
    private static final Logger LOGGER = LoggerFactory.getLogger(KafkaConfiguration.class);

    private final KafkaProperties properties;

    @Value("${spring.application.name}")
    private String context;

    @Value("${confluent.schema.registry.url}")
    private String schemaRegistry;

    public KafkaConfiguration(final KafkaProperties properties) {
        this.properties = properties;
    }

    @Bean
    public Map<String, Object> consumerConfigs()
    {
        Map<String, Object> props = properties.buildConsumerProperties();
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class);
        props.put(ProducerConfig.CLIENT_ID_CONFIG, context);
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, Boolean.FALSE);
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 1000);
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 5);
        props.put(AbstractKafkaSchemaSerDeConfig.KEY_SUBJECT_NAME_STRATEGY, TopicNameStrategy.class);
        props.put(AbstractKafkaSchemaSerDeConfig.VALUE_SUBJECT_NAME_STRATEGY, TopicRecordNameStrategy.class);
        props.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, schemaRegistry);
        props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);
        return props;
    }

    @Bean
    public DefaultKafkaConsumerFactory<Object, Object> consumerFactory() {
        KafkaAvroDeserializer kafkaAvroDeserializerForKey = new KafkaAvroDeserializer();
        kafkaAvroDeserializerForKey.configure(consumerConfigs(), true);
        KafkaAvroDeserializer kafkaAvroDeserializerForValue = new KafkaAvroDeserializer();
        kafkaAvroDeserializerForValue.configure(consumerConfigs(), false);

        ErrorHandlingDeserializer<Object> errorHandlingDeseserializerForKey = new ErrorHandlingDeserializer<>(
                kafkaAvroDeserializerForKey);
        ErrorHandlingDeserializer<Object> errorHandlingDeseserializerForValue = new ErrorHandlingDeserializer<>(
                kafkaAvroDeserializerForValue);

        return new DefaultKafkaConsumerFactory<>(consumerConfigs(), errorHandlingDeseserializerForKey,
                errorHandlingDeseserializerForValue);
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<KfKey, KfValue> kafkaListenerContainerFactory()
    {
        ConcurrentKafkaListenerContainerFactory<KfKey, KfValue> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE);
        factory.setAckDiscarded(true);

        return factory;
    }

    @Bean
    public ProducerFactory<KfKey, KfValue> producerFactory()
    {
        Map<String, Object> configProps = new HashMap<>();
        configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, properties.getProducer().getBootstrapServers());
        configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
        configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
        configProps.put(AbstractKafkaSchemaSerDeConfig.KEY_SUBJECT_NAME_STRATEGY, TopicNameStrategy.class);
        configProps.put(AbstractKafkaSchemaSerDeConfig.VALUE_SUBJECT_NAME_STRATEGY, TopicRecordNameStrategy.class);
        configProps.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, schemaRegistry);
        configProps.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);

        configProps.putAll(properties.getProducer().getProperties());

        return new DefaultKafkaProducerFactory<>(configProps);
    }

    @Bean
    public KafkaTemplate<KfKey, KfValue> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }

    @Bean
    public ProducerFactory<EivKey, EivValue> producerEivFactory()
    {
        Map<String, Object> configProps = new HashMap<>();
        configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, properties.getProducer().getBootstrapServers());
        configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
        configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
        configProps.put(AbstractKafkaSchemaSerDeConfig.KEY_SUBJECT_NAME_STRATEGY, TopicNameStrategy.class);
        configProps.put(AbstractKafkaSchemaSerDeConfig.VALUE_SUBJECT_NAME_STRATEGY, TopicRecordNameStrategy.class);
        configProps.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, schemaRegistry);
        configProps.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);

        configProps.putAll(properties.getProducer().getProperties());

        return new DefaultKafkaProducerFactory<>(configProps);
    }

    @Bean
    public KafkaTemplate<EivKey, EivValue> kafkaEivTemplate() {
        return new KafkaTemplate<>(producerEivFactory());
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<EivLegitimationVorgangKey, EivLegitimationVorgangValue> kafkaEivProcessListenerContainerFactory()
    {
        ConcurrentKafkaListenerContainerFactory<EivLegitimationVorgangKey, EivLegitimationVorgangValue> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE);
        factory.setAckDiscarded(true);

        return factory;
    }

    @Bean
    public ProducerFactory<DubKey, DubValue> producerDubFactory()
    {
        Map<String, Object> configProps = new HashMap<>();
        configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, properties.getProducer().getBootstrapServers());
        configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        configProps.put(AbstractKafkaSchemaSerDeConfig.KEY_SUBJECT_NAME_STRATEGY, TopicNameStrategy.class);
        configProps.put(AbstractKafkaSchemaSerDeConfig.VALUE_SUBJECT_NAME_STRATEGY, TopicRecordNameStrategy.class);

        configProps.putAll(properties.getProducer().getProperties());

        return new DefaultKafkaProducerFactory<>(configProps);
    }

    @Bean
    public KafkaTemplate<DubKey, DubValue> kafkaDubTemplate() {
        return new KafkaTemplate<>(producerDubFactory());
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<DubKey, DubValue> kafkaDubListenerContainerFactory()
    {
        ConcurrentKafkaListenerContainerFactory<DubKey, DubValue> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerDublettenbearbeitungFactory());
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL_IMMEDIATE);
        factory.setAckDiscarded(true);

        return factory;
    }

    @Bean
    public ProducerFactory<AAKey, AAValue> producerAfaFactory()
    {
        Map<String, Object> configProps = new HashMap<>();
        configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, properties.getProducer().getBootstrapServers());
        configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
        configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
        configProps.put(AbstractKafkaSchemaSerDeConfig.KEY_SUBJECT_NAME_STRATEGY, TopicNameStrategy.class);
        configProps.put(AbstractKafkaSchemaSerDeConfig.VALUE_SUBJECT_NAME_STRATEGY, TopicNameStrategy.class);
        configProps.put(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, schemaRegistry);
        configProps.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);

        configProps.putAll(properties.getProducer().getProperties());

        return new DefaultKafkaProducerFactory<>(configProps);
    }

    @Bean
    public KafkaTemplate<AAKey, AAValue> kafkaAfaTemplate() {
        return new KafkaTemplate<>(producerAfaFactory());
    }

    public ConsumerFactory<DubKey, DubValue> consumerDublettenbearbeitungFactory() {

        Map<String, Object> props = properties.buildConsumerProperties();
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        props.put(ProducerConfig.CLIENT_ID_CONFIG, context);
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, Boolean.FALSE);
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 1000);
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 5);
        props.put(AbstractKafkaSchemaSerDeConfig.KEY_SUBJECT_NAME_STRATEGY, TopicNameStrategy.class);
        props.put(AbstractKafkaSchemaSerDeConfig.VALUE_SUBJECT_NAME_STRATEGY, TopicRecordNameStrategy.class);

        return new DefaultKafkaConsumerFactory<>(props, new JsonDeserializer<>(DubKey.class)
                .forKeys()
                .ignoreTypeHeaders(),
                new JsonDeserializer<>(DubValue.class)
                .ignoreTypeHeaders());
    }
}
